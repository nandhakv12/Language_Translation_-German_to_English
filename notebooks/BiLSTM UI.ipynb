{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyOCfBFlfCWhmaCTOTbLFndM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ================================\n","# 1. Mount Drive & basic setup\n","# ================================\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import sentencepiece as spm\n","\n","DATA_DIR = Path(\"/content/drive/MyDrive/DL Final Project/data/europarl\")\n","CKPT_DIR = Path(\"/content/drive/MyDrive/DL Final Project/checkpoints\")\n","\n","SPM_PATH   = DATA_DIR / \"spm_bpe.model\"\n","FT_CKPT    = CKPT_DIR / \"seq2seq_finetuned_final.pth\"   # fine-tuned checkpoint you saved\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using device:\", device)\n","print(\"SPM model:\", SPM_PATH)\n","print(\"Checkpoint:\", FT_CKPT)"],"metadata":{"id":"574vveSnn9YU","executionInfo":{"status":"ok","timestamp":1764885336680,"user_tz":300,"elapsed":16917,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}},"outputId":"4debcd2c-b56d-4e4f-f2f4-8500bfdb4aa3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Using device: cuda\n","SPM model: /content/drive/MyDrive/DL Final Project/data/europarl/spm_bpe.model\n","Checkpoint: /content/drive/MyDrive/DL Final Project/checkpoints/seq2seq_finetuned_final.pth\n"]}]},{"cell_type":"code","source":["# ================================\n","# 2. Load SentencePiece tokenizer\n","# ================================\n","sp = spm.SentencePieceProcessor()\n","sp.Load(str(SPM_PATH))\n","\n","PAD, BOS, EOS, UNK = sp.pad_id(), sp.bos_id(), sp.eos_id(), sp.unk_id()\n","VOCAB = sp.get_piece_size()\n","print(\"Vocab size:\", VOCAB)\n","print(\"PAD/BOS/EOS/UNK:\", PAD, BOS, EOS, UNK)"],"metadata":{"id":"LHA-YR9Xn9a1","executionInfo":{"status":"ok","timestamp":1764885338976,"user_tz":300,"elapsed":2292,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}},"outputId":"7a609b5f-5aa0-4788-9eee-9fbe670455a3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size: 8000\n","PAD/BOS/EOS/UNK: 0 1 2 3\n"]}]},{"cell_type":"code","source":["# ================================\n","# 3. Model definitions (same as training)\n","# ================================\n","class Enc(nn.Module):\n","    def __init__(self, V, E=256, H=512):\n","        super().__init__()\n","        self.emb = nn.Embedding(V, E, padding_idx=PAD)\n","        self.rnn = nn.LSTM(E, H, num_layers=2, batch_first=True, bidirectional=True)\n","        self.H = H\n","\n","    def forward(self, x, lens):\n","        # x: [B, Ts]\n","        e = self.emb(x)\n","        pack = nn.utils.rnn.pack_padded_sequence(\n","            e, lens.cpu(), batch_first=True, enforce_sorted=False\n","        )\n","        out, (h, c) = self.rnn(pack)\n","        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n","        # concat bidirectional states\n","        h = torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], dim=2)\n","        c = torch.cat([c[0:c.size(0):2], c[1:c.size(0):2]], dim=2)\n","        return out, (h, c)\n","\n","class Luong(nn.Module):\n","    def __init__(self, H):\n","        super().__init__()\n","        self.W = nn.Linear(H, H, bias=False)\n","\n","    def forward(self, q, K, V, mask):\n","        # q: [B,H], K/V: [B,Ts,H]\n","        score = torch.bmm(self.W(q).unsqueeze(1), K.transpose(1, 2)).squeeze(1)\n","        score.masked_fill_(mask, -1e9)\n","        attn = score.softmax(-1)          # [B,Ts]\n","        ctx = torch.bmm(attn.unsqueeze(1), V).squeeze(1)  # [B,H]\n","        return ctx, attn\n","\n","class Dec(nn.Module):\n","    def __init__(self, V, E=256, H=1024):  # H doubled for bidi encoder\n","        super().__init__()\n","        self.emb = nn.Embedding(V, E, padding_idx=PAD)\n","        self.attn = Luong(H)\n","        self.rnn = nn.LSTM(E + H, H, num_layers=2, batch_first=True)\n","        self.fc  = nn.Linear(H + H, V)\n","        self.H = H\n","\n","    def forward(self, y_prev, hidden, enc_out, src_mask):\n","        # y_prev: [B,1]\n","        e = self.emb(y_prev)           # [B,1,E]\n","        q = hidden[0][-1]              # [B,H] (top-layer hidden)\n","        ctx, _ = self.attn(q, enc_out, enc_out, src_mask)\n","        rnn_in = torch.cat([e, ctx.unsqueeze(1)], dim=2)\n","        out, hidden = self.rnn(rnn_in, hidden)     # out: [B,1,H]\n","        logits = self.fc(torch.cat([out.squeeze(1), ctx], dim=1))  # [B,V]\n","        return logits, hidden\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, V, E=256, H=512):\n","        super().__init__()\n","        self.enc = Enc(V, E, H)\n","        self.dec = Dec(V, E, H * 2)\n","\n","    def mask(self, x):\n","        # x: [B,Ts]\n","        return (x == PAD)\n","\n","    def forward(self, src, src_lens, tgt, teacher=0.5):\n","        # Only needed for training; not used in UI notebook\n","        B, T = tgt.size()\n","        enc_out, hid = self.enc(src, src_lens)\n","        m = self.mask(src)\n","        y = tgt[:, 0].unsqueeze(1)     # BOS\n","        outs = []\n","        for t in range(1, T):\n","            logits, hid = self.dec(y, hid, enc_out, m)\n","            outs.append(logits.unsqueeze(1))\n","            use_teacher = torch.rand(1).item() < teacher\n","            next_tok = tgt[:, t] if use_teacher else logits.argmax(-1)\n","            y = next_tok.unsqueeze(1)\n","        return torch.cat(outs, 1)\n","\n","    def decode(self, src, src_lens, max_len=120):\n","        self.eval()\n","        enc_out, hid = self.enc(src, src_lens)\n","        m = self.mask(src)\n","        y = torch.full(\n","            (src.size(0), 1), BOS, dtype=torch.long, device=src.device\n","        )\n","        outs = [y]\n","        for _ in range(max_len):\n","            logits, hid = self.dec(y, hid, enc_out, m)\n","            y = logits.argmax(-1, keepdim=True)\n","            outs.append(y)\n","            if (y.squeeze(1) == EOS).all():\n","                break\n","        return torch.cat(outs, 1)\n"],"metadata":{"id":"etFlEQk-oMoc","executionInfo":{"status":"ok","timestamp":1764885338999,"user_tz":300,"elapsed":11,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ================================\n","# 4. Instantiate & load checkpoint\n","# ================================\n","model = Seq2Seq(VOCAB).to(device)\n","state = torch.load(FT_CKPT, map_location=device)\n","model.load_state_dict(state)\n","model.eval()\n","print(\"Fine-tuned model loaded.\")\n"],"metadata":{"id":"HW2LCTNeoMqn","executionInfo":{"status":"ok","timestamp":1764885351353,"user_tz":300,"elapsed":12355,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}},"outputId":"6d524877-f567-49dd-ab17-cf899072a85c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Fine-tuned model loaded.\n"]}]},{"cell_type":"code","source":["# ================================\n","# 5. Translation helper\n","# ================================\n","import re\n","\n","MAX_LEN = 120  # decode length (training used 120 inside SPMDataset)\n","\n","def clean_text(s: str) -> str:\n","    s = s.strip().lower()\n","    s = re.sub(r\"\\s+\", \" \", s)\n","    return s\n","\n","def strip_ids(ids):\n","    \"\"\"Remove BOS/PAD and cut at EOS.\"\"\"\n","    cleaned = []\n","    for i in ids:\n","        i = int(i)\n","        if i in (PAD, BOS):\n","            continue\n","        if i == EOS:\n","            break\n","        cleaned.append(i)\n","    return cleaned\n","\n","def translate_de_to_en(de_text: str) -> str:\n","    if not de_text.strip():\n","        return \"\"\n","    de_text = clean_text(de_text)\n","\n","    # Encode German sentence\n","    src_ids = [BOS] + sp.encode(de_text, out_type=int)[:MAX_LEN - 2] + [EOS]\n","    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n","    src_len = torch.tensor([len(src_ids)], dtype=torch.long, device=device)\n","\n","    with torch.no_grad():\n","        out = model.decode(src_tensor, src_len, max_len=MAX_LEN)  # [1, T]\n","    hyp_ids = strip_ids(out[0].tolist())\n","    en_text = sp.decode_ids(hyp_ids)\n","    return en_text\n","\n","# Quick sanity check (optional)\n","print(\"Test:\", translate_de_to_en(\"Das ist ein wichtiger Punkt.\"))\n"],"metadata":{"id":"AfJ0edlToMsk","executionInfo":{"status":"ok","timestamp":1764885351893,"user_tz":300,"elapsed":536,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}},"outputId":"6a1067bf-6ff5-4052-d69e-be8fedac30c2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Test: that is an important point .\n"]}]},{"cell_type":"code","source":["# ================================\n","# 6. Gradio web interface\n","# ================================\n","import gradio as gr\n","\n","def translate_interface(text):\n","    return translate_de_to_en(text)\n","\n","title = \"ðŸ‡©ðŸ‡ª âžœ ðŸ‡¬ðŸ‡§ GERMAN TO ENGLISH - Language Translation\"\n","description = (\n","    \"Bi-LSTM Encoderâ€“Decoder with Luong Attention for Germanâ€“English Translation \"\n",")\n","\n","demo = gr.Interface(\n","    fn=translate_interface,\n","    inputs=gr.Textbox(lines=4, label=\"German Input\"),\n","    outputs=gr.Textbox(lines=4, label=\"English Translation\"),\n","    title=title,\n","    description=description,\n",")\n","\n","# Set share=True to get a public URL that looks like a standalone website\n","demo.launch(share=True)"],"metadata":{"id":"w-w0Ejy5oMuj","executionInfo":{"status":"ok","timestamp":1764885362522,"user_tz":300,"elapsed":10627,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}},"outputId":"70fd61e8-6d0b-4f17-94f2-6fc7d3584e17","colab":{"base_uri":"https://localhost:8080/","height":609}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://ecb6494374330c5b71.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ecb6494374330c5b71.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"VbQdOGCsoMwP","executionInfo":{"status":"ok","timestamp":1764885362536,"user_tz":300,"elapsed":2,"user":{"displayName":"Kanisha R","userId":"01506756918863069258"}}},"execution_count":6,"outputs":[]}]}